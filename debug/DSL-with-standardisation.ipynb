{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8eca3f1f-e0c6-4a17-b2d8-ed9c948ab6f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "dsl v0.1.0 successfully loaded. See ?dsl for help. Note this is an early alpha release and backwards compatability may not be maintained.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "library(jsonlite)\n",
    "library(dsl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becd7060-24fa-468f-abb0-eb2d273c24ce",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Let:\n",
    "* $N$ be the total number of samples in the dataset\n",
    "* $n$ be the number of samples selected for expert annotation\n",
    "* $M_{true}$ be a logistic regression trained with expert annotations for all $N$ samples\n",
    "* $M_{sub}$ be a logistic regression trained with expert annotations on only the subset of $n$ samples selected for expert annotation\n",
    "* $M_{dsl}$ be a logistic regression trained with DSL using predicted annotations for all $N$ samples and expert annotations for a subset of $n$ samples\n",
    "\n",
    "As part of some experiments to try to measure the effective sample size of different datasets when using DSL (along the lines of [this paper](https://osf.io/preprints/socarxiv/j3bnt_v3)), we tried to compare the performance of $M_{sub}$ and $M_{dsl}$ when varying $n$. To this end, we measured the RMSE of the coefficients of both models compared to the coefficients of $M_{true}$. We expected that $M_{dsl}$ would always have a smaller RMSE than $M_{sub}$ and that in both cases the RMSE would go to 0 as $\\frac{n}{N} \\to 1$.\n",
    "\n",
    "However, for one of of the datasets we tested we observed a different trend: the RMSE for $M_{dsl}$ became larger than for $M_{sub}$ as we increased $n$ and it never converged to zero:\n",
    "\n",
    "![rmse.pdf](rmse.png)\n",
    "\n",
    "Our question is: **why is this happening?** So far we haven't been able to figure it out. Interestingly, these effects do not manifest in our other datasets (I included a second dataset based on Amazon reviews for which DSL behaves as expected).\n",
    "\n",
    "Below we provide an example of this phenomenon for the extreme case where $n = N = 10000$, where we expect that $M_{sub} = M_{dsl} = M_{true}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d2edfc-710c-4fd2-814a-05a78f5d1100",
   "metadata": {},
   "source": [
    "# The Data\n",
    "\n",
    "The data is a small subset of the [misinfo-general dataset](https://huggingface.co/datasets/ioverho/misinfo-general) of newspaper articles. We trimmed the dataset down to articles from two sources: The Guardian and The Sun. The goal of the logistic regression is to predict whether the article comes from The Guardian or The Sun based on certain independent variables of the text, such as the text length in words, the length of the title, and so forth. We balanced the dataset to have 5000 samples for each class. Predictions for the outcome label were obtained by training a classifier on the DistilBERT embeddings of the articles and their true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "225c728f-6599-422f-a35c-832deb25de1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"dataframe\">\n",
       "<caption>A data.frame: 6 × 6</caption>\n",
       "<thead>\n",
       "\t<tr><th></th><th scope=col>x1</th><th scope=col>x2</th><th scope=col>x3</th><th scope=col>x4</th><th scope=col>y</th><th scope=col>y_hat</th></tr>\n",
       "\t<tr><th></th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th><th scope=col>&lt;int&gt;</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>1</th><td>1595</td><td>235</td><td> 98</td><td>106</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>2</th><td>1562</td><td>237</td><td> 59</td><td> 78</td><td>0</td><td>1</td></tr>\n",
       "\t<tr><th scope=row>3</th><td>2574</td><td>410</td><td> 69</td><td> 95</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>4</th><td>1828</td><td>289</td><td> 37</td><td>102</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>5</th><td>1995</td><td>323</td><td> 88</td><td>118</td><td>0</td><td>0</td></tr>\n",
       "\t<tr><th scope=row>6</th><td>1852</td><td>300</td><td>115</td><td>119</td><td>0</td><td>0</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "A data.frame: 6 × 6\n",
       "\\begin{tabular}{r|llllll}\n",
       "  & x1 & x2 & x3 & x4 & y & y\\_hat\\\\\n",
       "  & <int> & <int> & <int> & <int> & <int> & <int>\\\\\n",
       "\\hline\n",
       "\t1 & 1595 & 235 &  98 & 106 & 0 & 0\\\\\n",
       "\t2 & 1562 & 237 &  59 &  78 & 0 & 1\\\\\n",
       "\t3 & 2574 & 410 &  69 &  95 & 0 & 0\\\\\n",
       "\t4 & 1828 & 289 &  37 & 102 & 0 & 0\\\\\n",
       "\t5 & 1995 & 323 &  88 & 118 & 0 & 0\\\\\n",
       "\t6 & 1852 & 300 & 115 & 119 & 0 & 0\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "A data.frame: 6 × 6\n",
       "\n",
       "| <!--/--> | x1 &lt;int&gt; | x2 &lt;int&gt; | x3 &lt;int&gt; | x4 &lt;int&gt; | y &lt;int&gt; | y_hat &lt;int&gt; |\n",
       "|---|---|---|---|---|---|---|\n",
       "| 1 | 1595 | 235 |  98 | 106 | 0 | 0 |\n",
       "| 2 | 1562 | 237 |  59 |  78 | 0 | 1 |\n",
       "| 3 | 2574 | 410 |  69 |  95 | 0 | 0 |\n",
       "| 4 | 1828 | 289 |  37 | 102 | 0 | 0 |\n",
       "| 5 | 1995 | 323 |  88 | 118 | 0 | 0 |\n",
       "| 6 | 1852 | 300 | 115 | 119 | 0 | 0 |\n",
       "\n"
      ],
      "text/plain": [
       "  x1   x2  x3  x4  y y_hat\n",
       "1 1595 235  98 106 0 0    \n",
       "2 1562 237  59  78 0 1    \n",
       "3 2574 410  69  95 0 0    \n",
       "4 1828 289  37 102 0 0    \n",
       "5 1995 323  88 118 0 0    \n",
       "6 1852 300 115 119 0 0    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data <- fromJSON(\"misinfo.json\")\n",
    "head(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c905e3d3-6fa2-4da5-b0b4-9cfc0d91feb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000 \n",
      "Class balance: 0.5 \n"
     ]
    }
   ],
   "source": [
    "cat(\"Number of samples:\", nrow(data), \"\\n\")\n",
    "cat(\"Class balance:\", sum(data$y) / nrow(data), \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d27243-6484-410a-8ee3-a4d49f3262fe",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a67468be-2c2e-42f2-94d3-dd6007f3798e",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED <- 0\n",
    "run <- function(data) {\n",
    "\n",
    "    cat(\"Setting the seed\\n\")\n",
    "    set.seed(SEED)\n",
    "\n",
    "    cat(\"Training the model with gold labels for all samples\\n\")\n",
    "    M_true <- glm(\n",
    "        y ~ x1 + x2 + x3 + x4,\n",
    "        data = data,\n",
    "        family = binomial\n",
    "    )\n",
    "\n",
    "    cat(\"Training the model with DSL\\n\")\n",
    "    M_dsl <- dsl(\n",
    "        model = \"logit\",\n",
    "        formula = y ~ x1 + x2 + x3 + x4,\n",
    "        predicted_var = \"y\",\n",
    "        prediction = \"y_hat\",\n",
    "        data = data,\n",
    "        seed = SEED\n",
    "    )\n",
    "\n",
    "    cat(\"Comparing the two models\\n\")\n",
    "    true_coeffs = M_true$coefficients\n",
    "    dsl_coeffs = M_dsl$coefficients\n",
    "    print(dsl_coeffs)\n",
    "    print(true_coeffs)\n",
    "    cat(\"RMSE:\", sqrt(mean((dsl_coeffs - true_coeffs)^2)), \"\\n\")\n",
    "    return(list(\"gold_model\" = M_true, \"dsl_model\" = M_dsl))\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b5a15b-5308-42d3-a888-7263d2ff2819",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "Since we actually have true labels for all samples when doing DSL, we expect the coefficients of both logistic regressions to be very similar, and therefore we expect both the standardised bias and the RMSE to be close to 0. However, we do not observe this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9ae4f28d-269c-46b7-b721-87ef0abfd2f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the seed\n",
      "Training the model with gold labels for all samples\n",
      "Training the model with DSL\n",
      "Cross-Fitting: 1/10..2/10..3/10..4/10..5/10..6/10..7/10..8/10..9/10..10/10..Comparing the two models\n",
      "  (Intercept)            x1            x2            x3            x4 \n",
      " 4.3469481141  0.0005172022  0.0033685361 -0.0228986108 -0.0715496354 \n",
      " (Intercept)           x1           x2           x3           x4 \n",
      " 3.904246329  0.007406862 -0.041126492 -0.026462945 -0.065385028 \n",
      "RMSE: 0.1990291 \n"
     ]
    }
   ],
   "source": [
    "models <- run(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b772ba1c-5c42-4b90-86bd-eefde00a6187",
   "metadata": {},
   "source": [
    "# Standardising the data\n",
    "\n",
    "Let's try the same thing again, but this time we mean-center the data first (giving it mean 0) before we do the rest. According to Claude, this could result in more sensitive intercepts ... https://claude.ai/chat/a64d67e3-fe1d-4579-8a92-48c8c2f53d85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "539a1a08-2c6a-4850-a2d6-c5f4cc0335b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the seed\n",
      "Training the model with gold labels for all samples\n",
      "Training the model with DSL\n",
      "Cross-Fitting: 1/10..2/10..3/10..4/10..5/10..6/10..7/10..8/10..9/10..10/10..Comparing the two models\n",
      "  (Intercept)            x1            x2            x3            x4 \n",
      " 0.0113065519  0.0005172022  0.0033685361 -0.0228986108 -0.0715496354 \n",
      " (Intercept)           x1           x2           x3           x4 \n",
      " 0.175669490  0.007406862 -0.041126492 -0.026462945 -0.065385028 \n",
      "RMSE: 0.07627995 \n"
     ]
    }
   ],
   "source": [
    "CENTER <- TRUE\n",
    "SCALE <- FALSE\n",
    "centered_data <- data\n",
    "centered_data$x1 <- scale(data$x1, center = CENTER, scale = SCALE)\n",
    "centered_data$x2 <- scale(data$x2, center = CENTER, scale = SCALE)\n",
    "centered_data$x3 <- scale(data$x3, center = CENTER, scale = SCALE)\n",
    "centered_data$x4 <- scale(data$x4, center = CENTER, scale = SCALE)\n",
    "scaled_models <- run(centered_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f74c716b-f1a4-444c-a722-197d849ef375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting the seed\n",
      "Training the model with gold labels for all samples\n",
      "Training the model with DSL\n",
      "Cross-Fitting: 1/10..2/10..3/10..4/10..5/10..6/10..7/10..8/10..9/10..10/10..Comparing the two models\n",
      "(Intercept)          x1          x2          x3          x4 \n",
      " 0.01130655  1.33780207  1.30924421 -1.51994741 -1.51239346 \n",
      "(Intercept)          x1          x2          x3          x4 \n",
      "  0.1756695  19.1586888 -15.9845760  -1.7565382  -1.3820879 \n",
      "RMSE: 11.1064 \n"
     ]
    }
   ],
   "source": [
    "CENTER <- TRUE\n",
    "SCALE <- TRUE\n",
    "scaled_data <- data\n",
    "scaled_data$x1 <- scale(data$x1, center = CENTER, scale = SCALE)\n",
    "scaled_data$x2 <- scale(data$x2, center = CENTER, scale = SCALE)\n",
    "scaled_data$x3 <- scale(data$x3, center = CENTER, scale = SCALE)\n",
    "scaled_data$x4 <- scale(data$x4, center = CENTER, scale = SCALE)\n",
    "scaled_models <- run(scaled_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
